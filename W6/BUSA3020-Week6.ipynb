{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and the Curse of Dimensionality {-}\n",
    "\n",
    "The **curse of dimensionality** refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (many features) that do not occur in low-dimensional settings such as the three-dimensional physical space. \n",
    "\n",
    "The expression was coined by Richard E. Bellman when considering problems in dynamic programming. \n",
    "- [https://en.wikipedia.org/wiki/Curse_of_dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
    "\n",
    "In Predictive Analytics/Machine Learning working in high-dimensional feature spaces can be undesirable for two reasons:\n",
    "1. Training a model / estimating parameters precisely requires a large amount of data -> length of the dataset required grows exponentially with the dimensionality (number of features)\n",
    "2. Making predictions from big datasets without dimensionality reduction is usually computationally intractable\n",
    "\n",
    "**Dimensionality reduction** is the transformation of data from a high-dimensional space (many features) into a low-dimensional space (fewer features) so that the low-dimensional representation retains some meaningful properties of the original data.\n",
    "\n",
    "In Week 5 we looked at two differnt techniques for Dimensionality Reduction:\n",
    "1. Regularization (L1 and L2)\n",
    "2. Feature Selection (Sequential Backward Selection)\n",
    "\n",
    "Both of these methods attempt reduce the dimension of the feature space by **picking relevant features and discarding others** via some optimization technique\n",
    "\n",
    "An alternative approach we consider this week is **feature extraction**:\n",
    "- **Summarise the information content** of a dataset by **transforming** the data into a new feature subspace of lower dimension\n",
    "    - This is a type of data compression where we attempt to extract the most relevant information from a dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Principal Component Analysis (PCA) - Unsupervised Dimensionality Reduction {-}\n",
    "\n",
    "- PCA is an **unsupervised linear** transformation technique used for dimensionality reduction\n",
    "    - PCA attempts to find orthogonal (right-angle/uncorrelated) features which explain most of the **variance** in high dimensional data\n",
    "        \n",
    "\n",
    "<img src=\"images/05_01.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "**PCA Method**\n",
    "- Start with $d$ features $\\mathbf{x}=\\left(\\begin{array}{cccc}\n",
    "x_1 & x_2 & \\dots & x_d\n",
    "\\end{array}\\right)$ where $x\\in \\mathbb{R}^{d}$ \n",
    "- Find a matrix $W\\in \\mathbb{R}^{d\\times k}$ where $k<d$ which will transform our data\n",
    "- Transform data $z=xW$ (this will transform one example/observation $x$)\n",
    "- New features are  $\\mathbf{z}=\\left(\\begin{array}{cccc}\n",
    "z_1 & z_2 & \\dots & z_k\n",
    "\\end{array}\\right)$ where the elements of $z\\in \\mathbb{R}^{k}$ are called **principal components**.\n",
    "- To transform entire dataset do $Z=XW$\n",
    "\n",
    "Notes:\n",
    "- The elements of $x$ are likely to be correlated (as any two random variables can be)\n",
    "- Because $W$ is constructed in a special way the elements of $z$ will be uncorrelated\n",
    "\n",
    "For instance if we wish to reduce 5 features ($d=5$) into 2 features ($k=2$) the transformation will be as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "### PCA Algorithm {-}\n",
    "\n",
    "So the key question is how do we find transformation matrix $W$\n",
    "\n",
    "The main steps behind PCA Algorithm are\n",
    "\n",
    "1. Standardise the original $d$-dimensional dataset\n",
    "2. Construct the covariance matrix $\\mathbf{\\Sigma}=\n",
    "\\left(\\begin{array}{cccc} \n",
    "\\sigma_1^{2} & \\sigma_{12} & \\dots & \\sigma_{1d}\\\\\n",
    "\\sigma_{21} & \\sigma_{2}^2 & \\dots & \\sigma_{2n}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "\\sigma_{d1} & \\sigma_{d2} & \\dots & \\sigma_{d}^2\n",
    "\\end{array}\\right)$\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues\n",
    "    - Find $d$ eigenvectors ($\\nu$) and $d$ corresponding eigenvalues ($\\lambda$) such that $\\Sigma \\nu=\\lambda \\nu$\n",
    "    - Methods to find $\\nu$ and $\\lambda$ to the above equation are taught in linear algebra courses\n",
    "4. Sort the eigenvalues by decreasing the order to rank the corresponding eigenvectors  \n",
    "    - Sort $\\lambda$s such that $\\lambda_1>\\lambda_2\\dots$ find correponding $\\nu_1, \\nu_2, \\dots$\n",
    "    - Note that $\\sum_{j=1}^d\\sigma_j^2=\\sum_{j=1}^d\\lambda_j$\n",
    "    - The $j$th principal component accounts for or \"explains\" $\\frac{\\lambda_j}{\\sum_{j=1}^d\\lambda_j}$ percent of the overall variability (this is called explained variance ratio)\n",
    "5. Select $k$ eigenvectors which correspond with the $k$ largest eigenvalues, where $k$ is the dimensionality of the new feature subspace ($k\\le d$)\n",
    "    - These $k$ eigenvectors contain most information (variance) of the original dataset\n",
    "6. Construct a projection matrix $W$ from the \"top\" $k$ eigenvectors\n",
    "7. Transform the $d$-dimensional input dataset $x$ using the projection matrix $W$ to obtain the new $k$-dimensional feature vector $z$. \n",
    "    - The elements of $z$ are called **principal components**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## Creating Principal Components {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "a) Import the Wine dataset from Week 5, split in into training and test (30%) datasets and standardize the data  \n",
    "\n",
    "```\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.4f}'.format \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# df = pd.read_csv('https://archive.ics.uci.edu/ml/'\n",
    "#                       'machine-learning-databases/wine/wine.data',\n",
    "#                       header=None)\n",
    "\n",
    "# df.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "#                    'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "#                    'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "#                    'Color intensity', 'Hue',\n",
    "#                    'OD280/OD315 of diluted wines', 'Proline']\n",
    "\n",
    "# df.to_excel('data/wine.xls', index = False)\n",
    "\n",
    "df = pd.read_excel('data/wine.xls')\n",
    "# df\n",
    "\n",
    "y, X = df.iloc[:, 0], df.iloc[:, 1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "X_train_scaled.shape\n",
    "\n",
    "print(df.info())\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "b) Use `np.cov` to compute the covariance matrix of `X_train_scaled`  \n",
    "- [https://numpy.org/doc/stable/reference/generated/numpy.cov.html](https://numpy.org/doc/stable/reference/generated/numpy.cov.html)  \n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "cov_mat = np.cov(X_train_scaled.T) # .T to transpose (flip) array\n",
    "\n",
    "print(cov_mat.shape)\n",
    "pd.DataFrame(cov_mat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "c) Use `np.linalg.eig` to compute eigenvalues and eigenvectors from the covariance matrix  \n",
    "- [https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html)  \n",
    "\n",
    "```\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print(eigen_vals.shape, eigen_vecs.shape)\n",
    "\n",
    "print(pd.Series(eigen_vals))\n",
    "pd.DataFrame(eigen_vecs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "d) \n",
    "- Plot explained variance ratios for each eigenvector.   \n",
    "- Also plot the cummulative sum explained by the first $i$ eigenvectors using `np.cumsum`   \n",
    "- [https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html) \n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "total_variance = sum(eigen_vals)\n",
    "var_exp = [(i / total_variance) for i in sorted(eigen_vals, reverse=True)]\n",
    "cummulative_var_exp = np.cumsum(var_exp)\n",
    "cummulative_var_exp\n",
    "\n",
    "\n",
    "plt.bar(range(1, 14), var_exp, alpha = 0.5, align = 'center', label = 'Individual Explained Variance')\n",
    "plt.step(range(1, 14), cummulative_var_exp, where='mid', label='Cummulative Explained Variance')\n",
    "plt.ylabel('Explained Variance Ratios')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "e) Create principal components $z_1$ and $z_2$ corresponding to 2 largest eigenvalues by following steps 5. - 7. above  \n",
    "\n",
    "```\n",
    "print(eigen_vecs.shape)\n",
    "\n",
    "W = eigen_vecs[:, :2]\n",
    "\n",
    "print(W)\n",
    "\n",
    "# Z = X_train_scaled.dot(W)\n",
    "Z = np.dot(X_train_scaled, W)\n",
    "\n",
    "print(Z.shape)\n",
    "print('First 2 PCs:\\n', Z[:10, :])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "f) $z_1$ and $z_2$ now represent our new features, label them according to $y$ and plot\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for lab, col, mark in zip(np.unique(y_train), colors, markers):\n",
    "    print(lab, col, mark)\n",
    "    plt.scatter(Z[y_train==lab, 0], Z[y_train==lab, 1], c = col, label = lab, marker = mark)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "### PCA in scikit-learn {-}\n",
    "\n",
    "PCA class in scikit-learn is a `transformer` class\n",
    "- `from sklearn.decomposition import PCA`\n",
    "1. Fit PCA using training data\n",
    "2. Transform training and test data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example - Wine Dataset**\n",
    "1. Use PCA from sklearn on Wine dataset\n",
    "2. Train LogisticRegression on the first 2 principal components\n",
    "3. Print forecast accuracy on both train and test datasets\n",
    "4. Plot decision boundaries for the train and test dataset using `plot_decision_regions` function we wrote previously\n",
    "5. Print explained variance ratios\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import plot_decision_regions as pdr\n",
    "\n",
    "# ----- Extract Principal Components\n",
    "pca = PCA(n_components = 2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# ----- Fit logistic regression\n",
    "lr = LogisticRegression(multi_class='ovr', random_state=1, solver='lbfgs')\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "print('Accuracy - Training dataset', lr.score(X_train_pca, y_train))\n",
    "\n",
    "\n",
    "pdr.plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Test Data\n",
    "\n",
    "print('Accuracy - Training dataset', lr.score(X_test_pca, y_test))\n",
    "pdr.plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#--------- Explained Variance Ratios\n",
    "\n",
    "pca2 = PCA(n_components = None)  # all principal components are kept\n",
    "X_train_pca2 = pca2.fit_transform(X_train_scaled)\n",
    "plt.bar(np.arange(1, pca2.explained_variance_ratio_.shape[0] + 1), pca2.explained_variance_ratio_)\n",
    "plt.xlabel('PC i')\n",
    "plt.ylabel('% of variance explained')\n",
    "plt.xticks(np.arange(1, pca2.explained_variance_ratio_.shape[0] + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Use PCA from sklearn on Wine dataset\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "import plot_decision_regions as pdr\n",
    "\n",
    "# ----- Extract Principal Components\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "X_train_pca[:10, :]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train LogisticRegression on the first 2 principal components\n",
    "\n",
    "```\n",
    "lr = LogisticRegression(multi_class='ovr', random_state=1, solver='lbfgs')\n",
    "\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "pdr.plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print forecast accuracy for both the training and test datasets\n",
    "\n",
    "```\n",
    "print(f'Accuracy - Training dataset {lr.score(X_train_pca, y_train):.3f}')\n",
    "print(f'Accuracy - Test dataset {lr.score(X_test_pca, y_test):.3f}')\n",
    "\n",
    "pdr.plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Print explained variance ratios\n",
    "\n",
    "```\n",
    "pca2 = PCA(n_components = None)  # all principal components are kept\n",
    "\n",
    "X_train_pca2 = pca2.fit_transform(X_train_scaled)\n",
    "\n",
    "plt.bar(np.arange(1, pca2.explained_variance_ratio_.shape[0] + 1), pca2.explained_variance_ratio_)\n",
    "\n",
    "plt.xlabel('PC i')\n",
    "plt.ylabel('% of variance explained')\n",
    "plt.xticks(np.arange(1, pca2.explained_variance_ratio_.shape[0] + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear Disciminant Analysis (LDA) {-}\n",
    "\n",
    "Both PCA and LDA are **linear transformation** techniques used to reduce the number of dimensions in a dataset.   \n",
    "\n",
    "However\n",
    "- PCA computes orthogonal/uncorrelated components that account for most of the total variance of the **features** without taking into account $y$ (unsupervised learning)\n",
    "- LDA finds features which optimize class separability - takes into account target variable $y$ -> supervised learning\n",
    "    \n",
    "LDA assumes:   \n",
    "- Data is normally distributed\n",
    "- Different classes have identical covariance matrices\n",
    "- Training examples (observations) are statistically independent of each other\n",
    "\n",
    "However, even if some assumptions are false LDA still performs reasonably well in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Algorithm {-}\n",
    "\n",
    "\n",
    "The main steps behind LDA are outlined below. For details and a step-by-step implementation in python see textbook.\n",
    "\n",
    "1. Standardise the original $d$-dimensional dataset\n",
    "2. For each **class** $i\\in\\{1, \\dots, c\\}$ compute the $d$-dimensional mean vector $\\mu_i=\\left(\\begin{array}{c} \n",
    "\\mu_{i,1}\\\\\n",
    "\\mu_{i,2}\\\\\n",
    "\\dots\\\\\n",
    "\\mu_{i,d}\n",
    "\\end{array}\\right)$\n",
    "\n",
    "3. For each class compute a covariance matrix of elements belonging to that class $\\Sigma_i=\\frac{1}{n_i}\\sum_{x\\in D_i}(x-\\mu_i)(x-\\mu_i)^T$ and then sum them to compute **within-class** scatter matrix $S_W =\\sum_{i=1}^c\\Sigma_i=(\\Sigma_1+\\dots+\\Sigma_c)$\n",
    "\n",
    "4. Compute the **between-class** scatter matrix $S_B=\\sum_{i=1}^{c}n_i(\\mu_i - \\mu)(\\mu_i-\\mu)^T$ where $\\mu$ is the vector of overall means for each feature (note: this is like a variance of the means)\n",
    "5. Compute the eigenvalues and corresponding eigenvectors of the matrix $S_W^{-1}S_B$\n",
    "6. Stack the $k$ eigenvectors that correspond to the $k$ largest eigenvalues as columns into a $d\\times d$ dimensional transformation matrix $W$\n",
    "7. Transform the $d$-dimensional input dataset $X$ using the projection matrix $W$ to obtain the new $k$-dimensional feature vector $Z=XW$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">    \n",
    "\n",
    "### LDA via Scikit-Learn {-}\n",
    "\n",
    "- As with PCA we will implement LDA via scikit-learn\n",
    "1. Train LDA transformer and extact 2 linear discriminents from the **training dataset** (now need to include y_train as well) \n",
    "2. Fit train LogisticRegression on the 2 LDs\n",
    "3. Print accuracy and plot decision regions\n",
    "4. Extract 2 LDs from the **test** dataset, print LR accuracy and plot decision regions\n",
    "\n",
    "```\n",
    "#--------------  1.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components = 2)\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n",
    "pd.DataFrame(X_train_lda)\n",
    "  \n",
    "#--------------  2.  \n",
    "lr = LogisticRegression(multi_class='ovr', random_state=1)\n",
    "lr.fit(X_train_lda, y_train)\n",
    "#--------------  3.  \n",
    "print('Accuracy - Training:', lr.score(X_train_lda, y_train))\n",
    "pdr.plot_decision_regions(X_train_lda, y_train, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_09.png', dpi=300)\n",
    "plt.show() \n",
    "#--------------  4.  \n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "print('Accuracy - Training:', lr.score(X_test_lda, y_test))\n",
    "\n",
    "pdr.plot_decision_regions(X_test_lda, y_test, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_10.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Train LDA transformer and extact 2 linear discriminents from the training dataset (now need to include y_train as well) \n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "lda = LDA(n_components = 2)\n",
    "\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train) # we used to call this Z above\n",
    "\n",
    "pd.DataFrame(X_train_lda)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit train LogisticRegression on the first 2 LDs\n",
    "\n",
    "```\n",
    "lr = LogisticRegression(multi_class='ovr', random_state=1)\n",
    "\n",
    "lr.fit(X_train_lda, y_train)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print accuracy and plot decision regions\n",
    "\n",
    "```\n",
    "print('Accuracy - Training:', lr.score(X_train_lda, y_train))\n",
    "\n",
    "\n",
    "pdr.plot_decision_regions(X_train_lda, y_train, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_09.png', dpi=300)\n",
    "plt.show() \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extract 2 LDs from the test dataset, print LR accuracy and plot decision regions\n",
    "\n",
    "```\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "print('Accuracy - Training:', lr.score(X_test_lda, y_test))\n",
    "\n",
    "pdr.plot_decision_regions(X_test_lda, y_test, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/05_10.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Kernel Principal Component Analysis (KPCA) for Nonlinear Mappings {-}\n",
    "\n",
    "In real world situations where data is often not linearly separable emplying PCA and LDA, which are linear transformation techniques for dimensionality reduction may not be the best practice.\n",
    "\n",
    "KPCA will transform data which is not linearly separable onto a new, lower-dimensional subspace which is linearly separable.\n",
    "\n",
    "<img src=\"images/05_11.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In SVM applications we used kernels to tackle nonlinear problems by transforming features to a higher dimensional space where the classes become linearly separable.\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">    \n",
    "\n",
    "### Kernel Function and the Kernel Trick {-}\n",
    "\n",
    "KPCA works as follows:\n",
    "1. Use a nonlinear function $\\phi$ to transform features onto a higher-dimensinal space\n",
    "    - Nonlinear mapping $\\phi: \\mathbb{R}^d\\rightarrow\\mathbb{R}^k$ where $k>d$\n",
    "    - For example $x=[x_1 \\quad x_2]^T \\quad \\underset{\\phi}{\\rightarrow} \\quad z =[x_1^2 \\quad \\sqrt{2x_1x_2} \\quad x_2^2]^T$ so here $d=2, k=3$\n",
    "2. Reduce dimension using standard PCA to project the data back onto a lower-dimensinal space where the data is linearly separable\n",
    "    - Compute similarity (kernel) of nonlinear functions in a high-dimensional space as the dot product of the nonlinear functions\n",
    "    \n",
    "Problem: Step 2 is very computationally expensive  \n",
    "Solution: **Kernel Trick**\n",
    "- Compute similarity (kernel) of nonlinear functions in a high-dimensional space as a nonlinear function of the dot product of the original features in a low-dimensional space\n",
    "- The order of operations is reversed\n",
    "    1. Computationally expensive: dot product of non-linear functions in a high-dimsional space\n",
    "    2. Computationally inexpensive - Kernel Trick: a nonlinear function of the dot product of original features in a low-dimensional space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">    \n",
    "\n",
    "### KPCA in scikit-learn {-}\n",
    "\n",
    "- We will not implement KPCA ourselves, but will use the libraries provided in scikit-learn\n",
    "    - For those more adventurous follow the steps in the textbook to build your own KPCA Python library\n",
    "    \n",
    "**Example 1: Separating Half-Moon Shapes**  \n",
    "**Example 2: Separating Concentric Circles**  \n",
    "a) Import concentric circles data data  \n",
    "```\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples = 1000, random_state = 123, noise=0.1, factor=0.2)\n",
    "\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(np.hstack((X, y.reshape(-1,1))), columns=['x1', 'x2', 'Class'])\n",
    "```\n",
    "b) Extract standard PCA and see if the classes are linearly separable  \n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "```\n",
    "c) Extract kernel PCA (rbf kernel) and see if classes are linearly separable  \n",
    "```\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.xlabel('K-PC1')\n",
    "plt.ylabel('K-PC2')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Example 1: Separating Half-Moon Shapes**  \n",
    "a) Import half-moons data  \n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "X, y = make_moons(n_samples = 100, random_state = 123)\n",
    "\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(np.hstack((X, y.reshape(-1,1))), columns=['x1', 'x2', 'Class'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Extract standard PCA and see if the classes are linearly separable  \n",
    "\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Extract kernel PCA (rbf kernel) and see if classes are linearly separable  \n",
    "\n",
    "```\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n",
    "\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.xlabel('K-PC1')\n",
    "plt.ylabel('K-PC2')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Example 2: Separating Concentric Circles**  \n",
    "a) Import concentric circles data data  \n",
    "\n",
    "```\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples = 1000, random_state = 123, noise=0.1, factor=0.2)\n",
    "\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(np.hstack((X, y.reshape(-1,1))), columns=['x1', 'x2', 'Class'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Extract standard PCA and see if the classes are linearly separable  \n",
    "\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "plt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "c) Extract kernel PCA (rbf kernel) and see if classes are linearly separable  \n",
    "\n",
    "```\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\n",
    "\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', marker='o', alpha=0.5)\n",
    "plt.xlabel('K-PC1')\n",
    "plt.ylabel('K-PC2')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
